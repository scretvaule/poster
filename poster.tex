% Unofficial University of Cambridge Poster Template
% https://github.com/andiac/gemini-cam
% a fork of https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[orientation=portrait,size=a0,scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{nott}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}
\usepackage{caption}
\usepackage{multirow}
\usepackage[table]{xcolor}

% \setbeamercolor{headline}{bg=lightblue}  % Pure white background
\setbeamercolor{headline}{bg=gray!25}               % 浅灰色

% Increase font size for portrait
\setbeamerfont{normal text}{size=\Large}
\setbeamerfont{block body}{size=\Large}
\setbeamerfont{itemize/enumerate body}{size=\Large}
\setbeamerfont{block alerted body}{size=\Large}
\setbeamerfont{block example body}{size=\Large}

% ====================
% Lengths
% ====================

\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.45\paperwidth}
\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

\newcommand{\sys}{AdaFuse}

% ====================
% Title
% ====================

\title{\textcolor{black}{\fontsize{40}{50}\selectfont\textbf{\sys}: Accelerating Dynamic Adapter Inference via\\Token-Level Pre-Gating and Fused Kernel Optimization}}


\author{\textcolor{black}{Qiyang Li${}^{1}$, Rui Kong${}^{1}$, Yuchen Li${}^{1}$, Hengyi Cai${}^{1}$, Shuaiqiang Wang${}^{1}$, Linghe Kong${}^{2}$, Guihai Chen${}^{2}$, Dawei Yin${}^{1}$}}

\institute[shortinst]{\textcolor{black}{${}^{1}$ Baidu Inc. \samelineand ${}^{2}$ Shanghai Jiao Tong University}}

% ====================
% Footer (optional)
% ====================

\footercontent{
  {\color{black}
    {AAAI 2026} \hfill
    % \href{mailto:yuchenli1230@gmail.com}{yuchenli1230@gmail.com}
  }
}
\logoright{\includegraphics[height=8cm]{logos/baidu.png}}
\logoleft{%
    \begin{minipage}[c]{8cm}
        \centering
        \includegraphics[height=6cm]{logos/sjtu.png}
    \end{minipage}
}
\begin{document}

\begin{frame}[t]

% ====================
% Row 1: Motivation & Contributions (Full Width)
% ====================
\begin{columns}[t]
\separatorcolumn
\begin{column}{\dimexpr2\colwidth+\sepwidth}
    \begin{exampleblock}{\textcolor{black}{Motivation \& Contributions}}
        \begin{minipage}{0.65\linewidth}
            \textbf{Problem \& Motivation:}
            \begin{itemize}
                \item Integrating dynamic sparse structures (MoE) with parameter-efficient adapters (e.g., LoRA) enhances LLMs but suffers from severe inference latency.
                \item As shown in the figure (right), decoding speed slows by \textbf{$>2.5\times$} compared to the backbone.
                \item The bottleneck is the \textbf{fragmented, sequential CUDA kernel launches} required for conventional dynamic routing, not the computation itself.
            \end{itemize}
            \vspace{1ex}
            \textbf{Our Key Contributions:}
            \begin{itemize}
                \item \textbf{Token-Level Pre-Gating:} A ``decide-once, apply-everywhere'' strategy. A single router at Layer 1 determines expert activation for all layers.
                \item \textbf{Fused Adapter Switching (SGMM):} A custom CUDA kernel that merges all selected LoRA adapters into the backbone in a \textbf{single efficient pass}.
                \item \textbf{Result:} \sys achieves SOTA accuracy while reducing latency by \textbf{2.4$\times$}.
            \end{itemize}
        \end{minipage}%
        \hfill
        \begin{minipage}{0.32\linewidth}
            \centering
            \includegraphics[width=0.95\linewidth]{figures/lora_vs_backbone_time_1_token.pdf}
            \captionof{figure}{\footnotesize Inference latency comparison: Backbone vs. Dynamic Adapters.}
        \end{minipage}
    \end{exampleblock}
\end{column}
\separatorcolumn
\end{columns}

% ====================
% Row 2: Framework Overview (Full Width)
% ====================
\begin{columns}[t]
\separatorcolumn
\begin{column}{\dimexpr2\colwidth+\sepwidth}
    \begin{block}{\sys: Framework Overview}
        \centering
        \begin{minipage}{0.65\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/overview.pdf}
        \end{minipage}%
        \hfill
        \begin{minipage}{0.32\linewidth}
            \vspace{1em}
            \textbf{Core Mechanisms:}
            \begin{itemize}
                \item \textbf{Token-wise Pre-gating (Fig. a):} Router $G^1$ at the first layer predicts expert activation for \textit{all} subsequent layers.
                \item \textbf{Decide-Once:} Eliminates $L-1$ redundant routing computations during inference.
                \item \textbf{Fused Switching (Fig. b):} Instead of loading adapters layer-by-layer, we use the \textbf{SGMM Kernel}.
                \item \textbf{SGMM Kernel:} \textit{Segmented Gather Matrix Multiplication}. It merges multiple LoRA adapters into the backbone weights in \textbf{one kernel launch} per layer.
            \end{itemize}
        \end{minipage}
    \end{block}
\end{column}
\separatorcolumn
\end{columns}

% ====================
% Row 3: Evaluation (Two Columns)
% ====================
\begin{columns}[t]
\separatorcolumn

% Left Column: Accuracy
\begin{column}{\colwidth}
    \begin{alertblock}{Evaluation: Accuracy}
        \textbf{1. General Capability Enhancement}
        
        \scriptsize
        \renewcommand{\arraystretch}{1.2}
        \setlength\tabcolsep{6pt}
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccccc}
        \toprule
        \textbf{Method} & \textbf{ARC} & \textbf{HellaSwag} & \textbf{MMLU} & \textbf{TruthfulQA} & \textbf{Avg} \\
        \midrule
        Llama2-7B (base) & 51.71 & \textbf{77.74} & 48.30 & 45.31 & 59.10 \\
        LoRA             & 51.79 & 77.02 & 50.46 & 45.13 & 59.64 \\
        MoRAL (layer-wise) & 52.13 & 77.57 & 51.10 & 45.93 & 60.22 \\
        PESC (block-wise) & \textbf{53.58} & 77.27 & 51.07 & 46.04 & \textbf{60.45} \\
        \rowcolor{gray!15} \textbf{\sys (Ours)} & 52.39 & 77.60 & \textbf{51.15} & \textbf{46.15} & 60.12 \\
        \bottomrule
        \end{tabular*}
        
        \vspace{1.5em}
        \normalsize
        \textbf{2. Domain-Specific Customization (Llama2-7B)}
        
        \scriptsize
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccc}
        \toprule
        \textbf{Method} & \textbf{ScienceQA} & \textbf{CSQA} & \textbf{OpenbookQA} & \textbf{Avg} \\
        \midrule
        Llama2-7B & 53.19 & 47.82 & 45.80 & 48.94 \\
        MoRAL & 90.74 & 76.41 & 76.60 & 81.25 \\
        PESC & 90.02 & 76.00 & 78.40 & 81.47 \\
        \rowcolor{gray!15} \textbf{\sys (Ours)} & 91.39 & \textbf{79.03} & 80.40 & \textbf{83.60} \\
        \bottomrule
        \end{tabular*}
        
        \vspace{1em}
        \normalsize
        \begin{itemize}
            \item \sys achieves competitive performance on general benchmarks.
            \item Outperforms baselines on reasoning-intensive tasks (e.g., CommonsenseQA), demonstrating that our efficiency optimizations do not compromise model quality.
        \end{itemize}
    \end{alertblock}
    
    \begin{block}{Conclusion}
        \sys introduces a novel \textbf{token-level pre-gating architecture} and \textbf{SGMM kernel optimization}. It effectively solves the inference latency bottleneck of dynamic adapters, offering SOTA accuracy with near-native inference speed, bridging the gap between capability and efficiency in production environments.
    \end{block}

\end{column}

\separatorcolumn

% Right Column: Efficiency
\begin{column}{\colwidth}

    \begin{block}{Runtime Performance \& Ablation}
        \centering
        
        \begin{minipage}{0.55\linewidth}
            \textbf{Decoding Latency \& Memory}
            \vspace{0.5em}
            
            \scriptsize
            \renewcommand{\arraystretch}{1.2}
            \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcc}
            \toprule
            \textbf{Method} & \textbf{Latency} & \textbf{Memory} \\
            & (ms/token) & (GiB) \\
            \midrule
            Llama2-7B & 2.4 & 12.9 \\
            MOLA & 25.3 (+954\%) & 26.3 (+104\%) \\
            PESC & 8.5 (+254\%) & 13.1 (+2\%) \\
            MoRAL & 8.6 (+258\%) & 13.3 (+3\%) \\
            \rowcolor{gray!25} \textbf{\sys} & \textbf{3.1 (+29\%)} & \textbf{13.8 (+7\%)} \\
            \bottomrule
            \end{tabular*}
        \end{minipage}%
        \hfill
        \begin{minipage}{0.42\linewidth}
            \textbf{Ablation: SGMM Kernel}
            \vspace{0.5em}
            
            \scriptsize
            \renewcommand{\arraystretch}{1.2}
            \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lc}
            \toprule
            \textbf{Variant} & \textbf{Latency} \\
            \midrule
            Llama2-7B & 2.4 \\
            MoRAL & 8.5 \\
            MoRAL (Simple) & 4.5 \\
            \sys (Simple) & 4.2 \\
            \rowcolor{gray!25} \textbf{\sys (SGMM)} & \textbf{3.1} \\
            \bottomrule
            \end{tabular*}
        \end{minipage}
        
        \vspace{1.5em}
        \includegraphics[width=0.95\linewidth]{figures/adapter_latency.pdf}
        \captionof{figure}{\textbf{Latency Comparison.} \sys (red) is significantly faster than other dynamic adapters (e.g., MoRAL, PESC) and close to the static backbone baseline.}
        
        \vspace{1em}
        \normalsize
        \begin{itemize}
            \item \sys is \textbf{2.7$\times$ faster} than PESC and only 29\% slower than the base model.
            \item \textbf{Ablation:} Without the SGMM kernel (Simple Merge), latency increases to 4.2ms, proving the necessity of our fused kernel optimization.
        \end{itemize}
    \end{block}

\end{column}
\separatorcolumn
\end{columns}

\end{frame}
\end{document}
