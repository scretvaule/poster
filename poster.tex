% Unofficial University of Cambridge Poster Template
% https://github.com/andiac/gemini-cam
% a fork of https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[orientation=portrait,size=a0,scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{nott}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}
\usepackage{caption}
\usepackage{multirow}
\usepackage[table]{xcolor}

% \setbeamercolor{headline}{bg=lightblue}  % Pure white background
\setbeamercolor{headline}{bg=gray!25}               % 浅灰色

% Increase font size for portrait
\setbeamerfont{normal text}{size=\Large}
\setbeamerfont{block body}{size=\Large}
\setbeamerfont{itemize/enumerate body}{size=\Large}
\setbeamerfont{block alerted body}{size=\Large}
\setbeamerfont{block example body}{size=\Large}

% Reduce vertical spacing between blocks
\addtobeamertemplate{block begin}{}{\vspace{-0.5em}}
\addtobeamertemplate{block end}{}{\vspace{-0.5em}}
\addtobeamertemplate{block alerted begin}{}{\vspace{-0.5em}}
\addtobeamertemplate{block alerted end}{}{\vspace{-0.5em}}
\addtobeamertemplate{block example begin}{}{\vspace{-0.5em}}
\addtobeamertemplate{block example end}{}{\vspace{-0.5em}}

% Tighter itemize spacing
\setlength{\itemsep}{0.3em}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}

% ====================
% Lengths
% ====================

\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.45\paperwidth}
\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

\newcommand{\sys}{AdaFuse}

% ====================
% Title
% ====================

\title{\textcolor{black}{\fontsize{55}{66}\selectfont\textbf{\sys}: Accelerating Dynamic Adapter Inference via\\Token-Level Pre-Gating and Fused Kernel Optimization}}


\author{\textcolor{black}{Qiyang Li${}^{1}$, Rui Kong${}^{1}$, Yuchen Li${}^{1}$, Hengyi Cai${}^{1}$, Shuaiqiang Wang${}^{1}$, Linghe Kong${}^{2}$, Guihai Chen${}^{2}$, Dawei Yin${}^{1}$}}

\institute[shortinst]{\textcolor{black}{${}^{1}$ Baidu Inc. \samelineand ${}^{2}$ Shanghai Jiao Tong University}}

% ====================
% Footer (optional)
% ====================

\footercontent{
  {\color{black}
    {AAAI 2026} \hfill
    \href{mailto:yuchenli1230@gmail.com}{yuchenli1230@gmail.com}
  }
}
\logoright{\includegraphics[height=8cm]{logos/baidu.png}}
\logoleft{%
    \begin{minipage}[c]{8cm}
        \centering
        \includegraphics[height=6cm]{logos/sjtu.png}
    \end{minipage}
}
\begin{document}

\begin{frame}[t]

% ====================
% Row 1: Motivation & Contributions (Full Width)
% ====================
\begin{columns}[t]
\separatorcolumn
\begin{column}{\dimexpr2\colwidth+\sepwidth}
    \begin{exampleblock}{\textcolor{black}{Motivation \& Contributions}}
        \begin{minipage}{0.60\linewidth}
            \textbf{\large Problem \& Motivation:}
            \begin{itemize}
                \item Integrating dynamic sparse structures (MoE) with parameter-efficient adapters (e.g., LoRA) enhances LLMs but suffers from severe inference latency.
                \item \textbf{Bottleneck:} Decoding speed slows by \textbf{$>2.5\times$} due to \textbf{fragmented, sequential CUDA kernel launches} required for dynamic routing (decide-then-load), not the computation itself.
            \end{itemize}
            \vspace{0.5em}
            \textbf{\large Our Key Contributions:}
            \begin{itemize}
                \item \textbf{Token-Level Pre-Gating:} A ``decide-once, apply-everywhere'' strategy where a single router determines expert activation for all layers.
                \item \textbf{Fused Adapter Switching (SGMM):} A custom CUDA kernel that merges selected LoRA adapters into the backbone in a \textbf{single efficient pass}.
                \item \textbf{Result:} \sys achieves SOTA accuracy while reducing latency by \textbf{2.4$\times$}, bridging the gap between capability and efficiency.
            \end{itemize}
        \end{minipage}%
        \hfill
        \begin{minipage}{0.38\linewidth}
            \centering
            % Increased figure size slightly
            \includegraphics[width=0.95\linewidth]{figures/lora_vs_backbone_time_1_token.pdf}
            \vspace{-0.5em}
            \captionof{figure}{\footnotesize Inference latency: Backbone vs. Dynamic Adapters.}
        \end{minipage}
    \end{exampleblock}
\end{column}
\separatorcolumn
\end{columns}

% ====================
% Row 2: Framework Overview (Full Width)
% ====================
\begin{columns}[t]
\separatorcolumn
\begin{column}{\dimexpr2\colwidth+\sepwidth}
    \begin{block}{\sys: Framework Overview}
        \begin{minipage}{0.62\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/overview.pdf}
        \end{minipage}%
        \hfill
        \begin{minipage}{0.35\linewidth}
            \vspace{1em}
            \textbf{\large Core Design:}
            \begin{itemize}
                \item \textbf{Token-wise Pre-gating (Fig. a):} Router $G^1$ at the first layer predicts expert activation for \textit{all} subsequent layers.
                \item \textbf{Decide-Once:} Eliminates $L-1$ redundant routing computations during inference.
                \item \textbf{Fused Switching (Fig. b):} Instead of loading adapters layer-by-layer, we use the \textbf{SGMM Kernel}.
                \item \textbf{Efficiency:} Transforms sequential operations into a single parallelized computation.
            \end{itemize}
        \end{minipage}
    \end{block}
\end{column}
\separatorcolumn
\end{columns}

% ====================
% Row 3: Main Content (Two Columns)
% ====================
\begin{columns}[t]
\separatorcolumn

% Left Column: SGMM + Accuracy
\begin{column}{\colwidth}
    
    % NEW BLOCK: SGMM Kernel Details to fill content
    \begin{block}{SGMM Kernel Optimization}
        \textbf{Segmented Gather Matrix Multiplication (SGMM)} is designed to execute batched GEMM operations efficiently.
        \begin{itemize}
            \item \textbf{Fused Operation:} Merges activated LoRA adapters into the backbone weights in one kernel launch:
            \begin{equation*}
                f^l_* = f^l + \text{Fused\_LoRA\_DOWN}^l \times \text{Fused\_LoRA\_UP}^l
            \end{equation*}
            \item \textbf{Optimization:} Divides large matrix multiplication into multiple GEMM tiles and uses a \textbf{pre-fetch buffer} mechanism to hide global memory loading latency.
            \item \textbf{Benefit:} Maximizes GPU thread block utilization and minimizes kernel launch overhead.
        \end{itemize}
    \end{block}

    \begin{alertblock}{Evaluation: Accuracy}
        \textbf{1. General Capability Enhancement}
        
        \scriptsize
        \renewcommand{\arraystretch}{1.2}
        \setlength\tabcolsep{6pt}
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccccc}
        \toprule
        \textbf{Method} & \textbf{ARC} & \textbf{HellaSwag} & \textbf{MMLU} & \textbf{TruthfulQA} & \textbf{Avg} \\
        \midrule
        Llama2-7B (base) & 51.71 & \textbf{77.74} & 48.30 & 45.31 & 59.10 \\
        LoRA             & 51.79 & 77.02 & 50.46 & 45.13 & 59.64 \\
        MoRAL            & 52.13 & 77.57 & 51.10 & 45.93 & 60.22 \\
        PESC             & \textbf{53.58} & 77.27 & 51.07 & 46.04 & \textbf{60.45} \\
        \rowcolor{gray!15} \textbf{\sys (Ours)} & 52.39 & 77.60 & \textbf{51.15} & \textbf{46.15} & 60.12 \\
        \bottomrule
        \end{tabular*}
        
        \vspace{0.5em}
        \normalsize
        \textbf{2. Domain-Specific Customization}
        
        \scriptsize
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccc}
        \toprule
        \textbf{Method} & \textbf{ScienceQA} & \textbf{CSQA} & \textbf{OpenbookQA} & \textbf{Avg} \\
        \midrule
        Llama2-7B & 53.19 & 47.82 & 45.80 & 48.94 \\
        MoRAL & 90.74 & 76.41 & 76.60 & 81.25 \\
        PESC & 90.02 & 76.00 & 78.40 & 81.47 \\
        \rowcolor{gray!15} \textbf{\sys (Ours)} & 91.39 & \textbf{79.03} & 80.40 & \textbf{83.60} \\
        \bottomrule
        \end{tabular*}
        
        \vspace{0.5em}
        \normalsize
        \begin{itemize}
            \item \sys outperforms baselines on reasoning-intensive tasks (e.g., CommonsenseQA), showing robust adaptation.
        \end{itemize}
    \end{alertblock}

\end{column}

\separatorcolumn

% Right Column: Runtime + Ablation + Conclusion
\begin{column}{\colwidth}

    \begin{block}{Runtime Performance}
        \textbf{Decoding Latency \& Memory (ShareGPT)}
        \vspace{0.5em}
        
        % Stacked table 1
        \scriptsize
        \renewcommand{\arraystretch}{1.2}
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcc}
        \toprule
        \textbf{Method} & \textbf{Latency (ms/token)} & \textbf{Memory (GiB)} \\
        \midrule
        Llama2-7B & 2.4 & 12.9 \\
        MOLA (Layer-wise) & 25.3 (+954\%) & 26.3 (+104\%) \\
        PESC (Block-wise) & 8.5 (+254\%) & 13.1 (+2\%) \\
        MoRAL (Layer-wise) & 8.6 (+258\%) & 13.3 (+3\%) \\
        \rowcolor{gray!25} \textbf{\sys} & \textbf{3.1 (+29\%)} & \textbf{13.8 (+7\%)} \\
        \bottomrule
        \end{tabular*}
        
        \vspace{0.5em}
        \centering
        \includegraphics[width=0.85\linewidth]{figures/adapter_latency.pdf}
        \captionof{figure}{\sys (red) is \textbf{2.7$\times$ faster} than PESC.}
    \end{block}
    
    \begin{block}{Ablation Study}
        \textbf{Impact of SGMM Kernel Optimization}
        \vspace{0.5em}
        
        % Stacked table 2
        \scriptsize
        \renewcommand{\arraystretch}{1.2}
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lc}
        \toprule
        \textbf{Variant} & \textbf{Latency (ms/token)} \\
        \midrule
        Llama2-7B (Base) & 2.4 \\
        MoRAL (Standard) & 8.5 \\
        MoRAL (Simple Merge) & 4.5 \\
        \sys (Simple Merge) & 4.2 \\
        \rowcolor{gray!25} \textbf{\sys (SGMM)} & \textbf{3.1} \\
        \bottomrule
        \end{tabular*}
        
        \vspace{0.5em}
        \normalsize
        \begin{itemize}
            \item Without the SGMM kernel (Simple Merge), latency increases to 4.2ms.
            \item This proves that \textbf{kernel-level fusion} is critical for efficiency, not just the routing strategy.
        \end{itemize}
    \end{block}
    
    \begin{alertblock}{Conclusion}
        \sys introduces a novel \textbf{token-level pre-gating architecture} and \textbf{SGMM kernel optimization}. It effectively solves the inference latency bottleneck of dynamic adapters, offering SOTA accuracy with near-native inference speed, bridging the gap between capability and efficiency.
    \end{alertblock}

    \begin{block}{References}
        \footnotesize
        [1] Wu et al., PESC, 2024. [2] Yang et al., MoRAL, 2024. [3] Gao et al., MOLA, 2024. [4] Touvron et al., Llama 2, 2023.
    \end{block}

\end{column}
\separatorcolumn
\end{columns}

\end{frame}
\end{document}
