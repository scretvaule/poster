@misc{chatgpt,
  title = {Chatgpt: Optimizing language models for dialogue.},
    author={OpenAI},
    year={2022},
    journal={OpenAI Blog},
  howpublished = {\url{https://openai.com/blog/chatgpt}},
}

@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}
}

@misc{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  year={2020},
  eprint={2005.14165},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2005.14165}
}


@article{Chowdhery2022PaLMSL,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark D{\'i}az and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  journal={J. Mach. Learn. Res.},
  year={2022},
  volume={24},
  pages={240:1-240:113},
  url={https://api.semanticscholar.org/CorpusID:247951931}
}


@article{Hoffmann2022TrainingCL,
  title={Training Compute-Optimal Large Language Models},
  author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and L. Sifre},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.15556},
  url={https://api.semanticscholar.org/CorpusID:247778764}
}

@article{hu2023llm,
  title={Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models},
  author={Hu, Zhiqiang and Lan, Yihuai and Wang, Lei and Xu, Wanyu and Lim, Ee-Peng and Lee, Roy Ka-Wei and Bing, Lidong and Poria, Soujanya},
  journal={arXiv preprint arXiv:2304.01933},
  year={2023}
}

@misc{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  year={2023},
  eprint={2302.13971},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2302.13971}
}


@article{Touvron2023Llama2O,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Hugo Touvron and Louis Martin and et al.},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.09288},
  url={https://api.semanticscholar.org/CorpusID:259950998}
}

@article{cai2024shortcut,
  title={Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts},
  author={Cai, Weilin and Jiang, Juyong and Qin, Le and Cui, Junwei and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2404.05019},
  year={2024}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Chung2022ScalingIL,
  title={Scaling Instruction-Finetuned Language Models},
  author={Hyung Won Chung and Le Hou and S. Longpre and Barret Zoph and Yi Tay and William Fedus and Eric Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Wei Yu and Vincent Zhao and Yanping Huang and Andrew M. Dai and Hongkun Yu and Slav Petrov and Ed Huai-hsin Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11416},
  url={https://api.semanticscholar.org/CorpusID:253018554}
}

@article{Iyer2022OPTIMLSL,
  title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  author={Srinivas Iyer and Xi Victoria Lin and Ramakanth Pasunuru and Todor Mihaylov and Daniel Simig and Ping Yu and Kurt Shuster and Tianlu Wang and Qing Liu and Punit Singh Koura and Xian Li and Brian O'Horo and Gabriel Pereyra and Jeff Wang and Christopher Dewan and Asli Celikyilmaz and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.12017},
  url={https://api.semanticscholar.org/CorpusID:255096269}
}


@Misc{peft,
  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}

@article{Xiao2023OffsiteTuningTL,
  title={Offsite-Tuning: Transfer Learning without Full Model},
  author={Guangxuan Xiao and Ji Lin and Song Han},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.04870},
  url={https://api.semanticscholar.org/CorpusID:256697131}
}

@article{Frantar2022GPTQAP,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.17323},
  url={https://api.semanticscholar.org/CorpusID:253237200}
}

@article{xiao2023smoothquant,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Demouth, Julien and Han, Song},
  journal={arXiv preprint arXiv:2211.10438},
  year={2022}
}

@article{Frantar2023SparseGPTML,
  title={SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
  author={Elias Frantar and Dan Alistarh},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.00774},
  url={https://api.semanticscholar.org/CorpusID:255372747}
}

@article{Ma2023LLMPrunerOT,
  title={LLM-Pruner: On the Structural Pruning of Large Language Models},
  author={Xinyin Ma and Gongfan Fang and Xinchao Wang},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.11627},
  url={https://api.semanticscholar.org/CorpusID:258823276}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}


@inproceedings{he2022towards,
title={Towards a Unified View of Parameter-Efficient Transfer Learning},
author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=0RDcd5Axok}
}


@misc{stickland2019bert,
      title={BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning}, 
      author={Asa Cooper Stickland and Iain Murray},
      year={2019},
      eprint={1902.02671},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{Hu2021LoRALA,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021},
  eprint={2106.09685},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2106.09685}
}

@article{Huang2023LoraHubEC,
  title={LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition},
  author={Chengsong Huang and Qian Liu and Bill Yuchen Lin and Tianyu Pang and Chao Du and Min Lin},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.13269},
  url={https://api.semanticscholar.org/CorpusID:260155012}
}



@article{Li2021PrefixTuningOC,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Xiang Lisa Li and Percy Liang},
  journal={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year={2021},
  volume={abs/2101.00190},
  url={https://api.semanticscholar.org/CorpusID:230433941}
}

@inproceedings{Lester2021ThePO,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Brian Lester and Rami Al-Rfou and Noah Constant},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:233296808}
}

@article{Gu2021PPTPP,
  title={PPT: Pre-trained Prompt Tuning for Few-shot Learning},
  author={Yuxian Gu and Xu Han and Zhiyuan Liu and Minlie Huang},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.04332},
  url={https://api.semanticscholar.org/CorpusID:237452236}
}


@article{Wang2023MultitaskPT,
  title={Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning},
  author={Zhen Wang and Rameswar Panda and Leonid Karlinsky and Rog{\'e}rio Schmidt Feris and Huan Sun and Yoon Kim},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.02861},
  url={https://api.semanticscholar.org/CorpusID:257365136}
}

@article{BenZaken2021BitFitSP,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Elad Ben-Zaken and Shauli Ravfogel and Yoav Goldberg},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.10199},
  url={https://api.semanticscholar.org/CorpusID:231672601}
}

@article{Liu2022FewShotPF,
  title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning},
  author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.05638},
  url={https://api.semanticscholar.org/CorpusID:248693283}
}

@inproceedings{shazeer2017outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  booktitle={International Conference on Learning Representations},
  year={2017}
}


@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}

@inproceedings{lepikhin2021gshard,
  title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  booktitle={International Conference on Learning Representations},
  year={2021}
}


@inproceedings{zuo2022taming,
  title={Taming Sparsely Activated Transformer with Stochastic Experts},
  author={Zuo, Simiao and Liu, Xiaodong and Jiao, Jian and Kim, Young Jin and Hassan, Hany and Zhang, Ruofei and Gao, Jianfeng and Zhao, Tuo},
  booktitle={International Conference on Learning Representations},
  year={2022}
}


@article{liu2023moelora,
  title={MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications},
  author={Qidong Liu and Xian Wu and Xiangyu Zhao and Yuanshao Zhu and Derong Xu and Feng Tian and Yefeng Zheng},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.18339},
  url={https://api.semanticscholar.org/CorpusID:264590549}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@article{Babakniya2023SLoRAFP,
  title={SLoRA: Federated Parameter Efficient Fine-Tuning of Language Models},
  author={Sara Babakniya and Ahmed Roushdy Elkordy and Yahya H. Ezzeldin and Qingfeng Liu and Kee-Bong Song and Mostafa El-Khamy and Salman Avestimehr},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.06522},
  url={https://api.semanticscholar.org/CorpusID:260887495}
}

@article{Ye2023ASPENHL,
  title={ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU},
  author={Zhengmao Ye and Dengchun Li and Jingqi Tian and Tingfeng Lan and Jie Zuo and Lei Duan and Hui Lu and Yexi Jiang and Jian Sha and Ke Zhang and Mingjie Tang},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.02515},
  url={https://api.semanticscholar.org/CorpusID:265659414}
}

@misc{wu2024parameterefficient,
      title={Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks}, 
      author={Haoyuan Wu and Haisheng Zheng and Bei Yu},
      year={2024},
      eprint={2401.02731},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{shoeybi2020megatronlm,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{iyer2022opt,
  title={Opt-iml: Scaling language model instruction meta learning through the lens of generalization},
  author={Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
  journal={arXiv preprint arXiv:2212.12017},
  year={2022}
}

@article{pfeiffer2020adapterfusion,
  title={Adapterfusion: Non-destructive task composition for transfer learning},
  author={Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2005.00247},
  year={2020}
}



@inproceedings{rajbhandari2022deepspeed-moe,
author = {Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Yazdani Aminabadi, Reza and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
title = {DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale},
booktitle = {ICML 2022},
year = {2022},
month = {January},
abstract = {As the training of giant dense models hits the boundary on the availability and capability of the hardware resources today, Mixture-of-Experts (MoE) models become one of the most promising model architectures due to their significant training cost reduction compared to a quality-equivalent dense model. Its training cost saving is demonstrated from encoder-decoder models (prior works) to a 5x saving for auto-aggressive language models (this work along with parallel explorations). However, due to the much larger model size and unique architecture, how to provide fast MoE model inference remains challenging and unsolved, limiting its practical usage. To tackle this, we present DeepSpeed-MoE, an end-to-end MoE training and inference solution as part of the DeepSpeed library, including novel MoE architecture designs and model compression techniques that reduce MoE model size by up to 3.7x, and a highly optimized inference system that provides 7.3x better latency and cost compared to existing MoE inference solutions. DeepSpeed-MoE offers an unprecedented scale and efficiency to serve massive MoE models with up to 4.5x faster and 9x cheaper inference compared to quality-equivalent dense models. We hope our innovations and systems help open a promising path to new directions in the large model landscape, a shift from dense to sparse MoE models, where training and deploying higher-quality models with fewer resources becomes more widely possible.},
}

@article{yang2024moral,
  title={MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning},
  author={Yang, Shu and Ali, Muhammad Asif and Wang, Cheng-Long and Hu, Lijie and Wang, Di},
  journal={arXiv preprint arXiv:2402.11260},
  year={2024}
}

@article{wu2024parameter,
  title={Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks},
  author={Wu, Haoyuan and Zheng, Haisheng and Yu, Bei},
  journal={arXiv preprint arXiv:2401.02731},
  year={2024}
}

@misc{dou2024loramoe,
      title={LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin}, 
      author={Shihan Dou and Enyu Zhou and Yan Liu and Songyang Gao and Jun Zhao and Wei Shen and Yuhao Zhou and Zhiheng Xi and Xiao Wang and Xiaoran Fan and Shiliang Pu and Jiang Zhu and Rui Zheng and Tao Gui and Qi Zhang and Xuanjing Huang},
      year={2024},
      eprint={2312.09979},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gao2024higher,
      title={Higher Layers Need More LoRA Experts}, 
      author={Chongyang Gao and Kezhen Chen and Jinmeng Rao and Baochen Sun and Ruibo Liu and Daiyi Peng and Yawen Zhang and Xiaoyuan Guo and Jie Yang and VS Subrahmanian},
      year={2024},
      eprint={2402.08562},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{luo2024moelora,
      title={MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models}, 
      author={Tongxu Luo and Jiahe Lei and Fangyu Lei and Weihao Liu and Shizhu He and Jun Zhao and Kang Liu},
      year={2024},
      eprint={2402.12851},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gou2024mixture,
      title={Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning}, 
      author={Yunhao Gou and Zhili Liu and Kai Chen and Lanqing Hong and Hang Xu and Aoxue Li and Dit-Yan Yeung and James T. Kwok and Yu Zhang},
      year={2024},
      eprint={2312.12379},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{feng2024mixtureofloras,
      title={Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models}, 
      author={Wenfeng Feng and Chuzhan Hao and Yuewei Zhang and Yu Han and Hao Wang},
      year={2024},
      eprint={2403.03432},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shen2024multimodal,
      title={Multimodal Instruction Tuning with Conditional Mixture of LoRA}, 
      author={Ying Shen and Zhiyang Xu and Qifan Wang and Yu Cheng and Wenpeng Yin and Lifu Huang},
      year={2024},
      eprint={2402.15896},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{geva2021transformer,
      title={Transformer Feed-Forward Layers Are Key-Value Memories}, 
      author={Mor Geva and Roei Schuster and Jonathan Berant and Omer Levy},
      year={2021},
      eprint={2012.14913},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zoph2022stmoe,
      title={ST-MoE: Designing Stable and Transferable Sparse Expert Models}, 
      author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam Shazeer and William Fedus},
      year={2022},
      eprint={2202.08906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yeh2024navigating,
      title={Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation}, 
      author={Shih-Ying Yeh and Yu-Guan Hsieh and Zhidong Gao and Bernard B W Yang and Giyeong Oh and Yanmin Gong},
      year={2024},
      eprint={2309.14859},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ponti2022combining,
      title={Combining Modular Skills in Multitask Learning}, 
      author={Edoardo M. Ponti and Alessandro Sordoni and Yoshua Bengio and Siva Reddy},
      year={2022},
      eprint={2202.13914},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kopiczko2024vera,
      title={VeRA: Vector-based Random Matrix Adaptation}, 
      author={Dawid J. Kopiczko and Tijmen Blankevoort and Yuki M. Asano},
      year={2024},
      eprint={2310.11454},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hyeonwoo2023fedpara,
      title={FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning}, 
      author={Nam Hyeon-Woo and Moon Ye-Bin and Tae-Hyun Oh},
      year={2023},
      eprint={2108.06098},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2023adalora,
      title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}, 
      author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
      year={2023},
      eprint={2303.10512},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{renduchintala2023tiedlora,
      title={Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying}, 
      author={Adithya Renduchintala and Tugrul Konuk and Oleksii Kuchaiev},
      year={2023},
      eprint={2311.09578},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2024dora,
      title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
      author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
      year={2024},
      eprint={2402.09353},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{yeh2023navigating,
  title={Navigating text-to-image customization: From lycoris fine-tuning to model evaluation},
  author={Yeh, Shin-Ying and Hsieh, Yu-Guan and Gao, Zhidong and Yang, Bernard BW and Oh, Giyeong and Gong, Yanmin},
  journal={arXiv preprint arXiv:2309.14859},
  year={2023}
}

@inproceedings{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      booktitle={Advances in Neural Information Processing Systems 30 (NIPS 2017)},
      year={2017},
      pages={5998--6008},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2024llavamole,
      title={LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs}, 
      author={Shaoxiang Chen and Zequn Jie and Lin Ma},
      year={2024},
      eprint={2401.16160},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{clark2018think,
      title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, 
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{mihaylov2018suit,
      title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}, 
      author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
      year={2018},
      eprint={1809.02789},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{clark2019boolq,
      title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}, 
      author={Christopher Clark and Kenton Lee and Ming-Wei Chang and Tom Kwiatkowski and Michael Collins and Kristina Toutanova},
      year={2019},
      eprint={1905.10044},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bisk2019piqa,
      title={PIQA: Reasoning about Physical Commonsense in Natural Language}, 
      author={Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
      year={2019},
      eprint={1911.11641},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mao2022unipelt,
      title={UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning}, 
      author={Yuning Mao and Lambert Mathias and Rui Hou and Amjad Almahairi and Hao Ma and Jiawei Han and Wen-tau Yih and Madian Khabsa},
      year={2022},
      eprint={2110.07577},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2024mixlora,
      title={MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA based Mixture of Experts}, 
      author={Dengchun Li and Yingzi Ma and Naizheng Wang and Zhiyuan Cheng and Lei Duan and Jie Zuo and Cal Yang and Mingjie Tang},
      year={2024},
      eprint={2404.15159},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@online{xAI-2024,
  author = "xAI",
  year = "2024",
  title = "Open release of grok-1",
  url = "https://x.ai/blog/grok-os",
  urldate = "2024-03-17",
}

@misc{dbrx2024,
  author = {{The Mosaic Research Team}},
  title = {Introducing dbrx: A New State-of-the-Art Open LLM},
  year = {2024},
  howpublished = {\url{https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm}},
  note = {Accessed on April 26, 2024}
}

@misc{artic2024,
  author = {{Snowflake AI Research Team}},
  title = {Snowflake Arctic: The Best LLM for Enterprise AI — Efficiently Intelligent, Truly Open},
  year = {2024},
  note = {Accessed on April 26, 2024}
}

@misc{openchat2023sharegpt4,
  title        = {ShareGPT4 Dataset},
  author       = {{OpenChat}},
  howpublished = {Hugging Face Datasets},
  year         = {2023},
  url          = {https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset/blob/main/sharegpt_clean.json},
  note         = {Accessed: 2024-05-11}
}

@misc{SlimOrca,
  title = {SlimOrca: An Open Dataset of GPT-4 Augmented FLAN Reasoning Traces, with Verification},
  author = {Wing Lian and Guan Wang and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and "Teknium"},
  year = {2023},
  publisher = {HuggingFace},
  url = {https://huggingface.co/Open-Orca/SlimOrca}
}

@article{wei2023magicoder,
  title={Magicoder: Source Code Is All You Need},
  author={Wei, Yuxiang and Wang, Zhe and Liu, Jiawei and Ding, Yifeng and Zhang, Lingming},
  journal={arXiv preprint arXiv:2312.02120},
  year={2023}
}

@article{yu2023metamath,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@article{Clark2018ThinkYH,
  title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.05457}
}

@article{hendryckstest2021,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@inproceedings{lin-etal-2022-truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.229",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
}

@article{sakaguchi2019winogrande,
    title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
    author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
    journal={arXiv preprint arXiv:1907.10641},
    year={2019}
}

@inproceedings{lu2022learn,
    title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
    author={Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},
    booktitle={The 36th Conference on Neural Information Processing Systems (NeurIPS)},
    year={2022}
}

@inproceedings{talmor-etal-2019-commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
}

@inproceedings{OpenBookQA2018,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}

@article{cai2024medusa,
  title={Medusa: Simple llm inference acceleration framework with multiple decoding heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}

@inproceedings{Miao_2024, series={ASPLOS '24},
   title={SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification},
   url={http://dx.doi.org/10.1145/3620666.3651335},
   DOI={10.1145/3620666.3651335},
   booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
   publisher={ACM},
   author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and Shi, Chunan and Chen, Zhuoming and Arfeen, Daiyaan and Abhyankar, Reyna and Jia, Zhihao},
   year={2024},
   month=apr, collection={ASPLOS ’24} }


@misc{loshchilov2019decoupled,
   title={Decoupled Weight Decay Regularization}, 
   author={Ilya Loshchilov and Frank Hutter},
   year={2019},
   eprint={1711.05101},
   archivePrefix={arXiv},
   primaryClass={cs.LG}
}

@misc{dettmers2023qlora,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{zhang2023llama,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16199},
  year={2023}
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@misc{liu2023gpt,
      title={GPT Understands, Too}, 
      author={Xiao Liu and Yanan Zheng and Zhengxiao Du and Ming Ding and Yujie Qian and Zhilin Yang and Jie Tang},
      year={2023},
      eprint={2103.10385},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2023punica,
      title={Punica: Multi-Tenant LoRA Serving}, 
      author={Lequn Chen and Zihao Ye and Yongji Wu and Danyang Zhuo and Luis Ceze and Arvind Krishnamurthy},
      year={2023},
      eprint={2310.18547},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{CUTLASS2023,
  title        = {CUTLASS},
  author       = {Vijay Thakkar and Pradeep Ramani and Cris Cecka and Aniket Shivam and Honghao Lu and Ethan Yan and Jack Kosaian and Mark Hoemmen and Haicheng Wu and Andrew Kerr and Matt Nicely and Duane Merrill and Dustyn Blasig and Fengqi Qiao and Piotr Majcher and Paul Springer and Markus Hohnerbach and Jin Wang and Manish Gupta},
  organization = {NVIDIA},
  address      = {Google for Manish Gupta},
  year         = 2023,
  version      = {3.0.0},
  date         = {2023-01-23},
  howpublished = {\url{https://github.com/NVIDIA/cutlass}},
  abstract     = {CUTLASS is a collection of CUDA C++ template abstractions for implementing high-performance matrix-multiplication (GEMM) and related computations at all levels and scales within CUDA. It incorporates strategies for hierarchical decomposition and data movement similar to those used to implement cuBLAS and cuDNN.},
  keywords     = {cutlass, tensor cores, cuda, cute, nvidia, gpu, linear algebra, matrix computations},
  license      = {BSD-3-Clause},
  license-url  = {https://github.com/NVIDIA/cutlass/blob/v3.0.0/LICENSE.txt}
}

@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{eigen2013learning,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  booktitle={arXiv preprint arXiv:1312.4314},
  year={2013}
}

@article{du2022glam,
  title={GLaM: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  journal={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}

@article{anil2023palm,
  title={PaLM 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@article{lepikhin2020gshard,
  title={GShard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@article{lewis2021base,
  title={BASE layers: Simplifying training of large, sparse models},
  author={Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  journal={International Conference on Machine Learning},
  pages={6265--6274},
  year={2021},
  organization={PMLR}
}

@article{roller2021hash,
  title={Hash layers for large sparse models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}

@inproceedings{rücklé2021adapterdrop,
    title = "{A}dapter{D}rop: On the Efficiency of Adapters in Transformers",
    author = "R{\"u}ckl{\'e}, Andreas and  Pfeiffer, Jonas and  Trill, Simon and  Kamath, Aishwarya and  Sturm, Tristan and  Sachan, Mrinal and  Gurevych, Iryna",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.15",
    pages = "187--200"
}

@inproceedings{mahabadi2021compacter,
    title = "{C}ompacter: Efficient Low-Rank Information Transfer For Parameter-Efficient Tunning",
    author = "Mahabadi, Rabeeh Karimi and  Henderson, James and  Dehghani, Mostafa and  Daval-Frerot, Gweltaz and  Peyre, J{\'e}r{\'e}mie and  Trill, Simon and  Sachan, Mrinal",
    booktitle = "Thirty-fifth Conference on Neural Information Processing Systems",
    year = "2021",
    url = "https://openreview.net/forum?id=OQ08TThxS2"
}

@article{li2025towards,
  title={Towards AI Search Paradigm},
  author={Li, Yuchen and Cai, Hengyi and Kong, Rui and Chen, Xinran and Chen, Jiamin and Yang, Jun and Zhang, Haojie and Li, Jiayi and Wu, Jiayi and Chen, Yiqun and others},
  journal={arXiv preprint arXiv:2506.17188},
  year={2025}
}

@inproceedings{chen2025multi,
  title={Multi-Agent Proactive Information Seeking with Adaptive LLM Orchestration for Non-Factoid Question Answering},
  author={Chen, Xinran and Li, Yuchen and Cai, Hengyi and Ma, Zhuoran and Chen, Xuanang and Xiong, Haoyi and Wang, Shuaiqiang and He, Ben and Sun, Le and Yin, Dawei},
  booktitle={Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2},
  pages={4341--4352},
  year={2025}
}

@article{xiong2024search,
  title={When search engine services meet large language models: visions and challenges},
  author={Xiong, Haoyi and Bian, Jiang and Li, Yuchen and Li, Xuhong and Du, Mengnan and Wang, Shuaiqiang and Yin, Dawei and Helal, Sumi},
  journal={IEEE Transactions on Services Computing},
  year={2024},
  publisher={IEEE}
}

@article{wang2024enhancing,
  title={Enhancing code llms with reinforcement learning in code generation: A survey},
  author={Wang, Junqiao and Zhang, Zeng and He, Yangfan and Zhang, Zihao and Song, Xinyuan and Song, Yuyang and Shi, Tianyu and Li, Yuchen and Xu, Hengyuan and Wu, Kunyu and others},
  journal={arXiv preprint arXiv:2412.20367},
  year={2024}
}

@inproceedings{li2025m,
  title={M 2 oERank: Multi-Objective Mixture-of-Experts Enhanced Ranking for Satisfaction-Oriented Web Search},
  author={Li, Yuchen and Zhang, Hao and Zhang, Yongqi and Ma, Xinyu and Ye, Wenwen and Song, Naifei and Wang, Shuaiqiang and Xiong, Haoyi and Yin, Dawei and Chen, Lei},
  booktitle={2025 IEEE 41st International Conference on Data Engineering (ICDE)},
  pages={4441--4454},
  year={2025},
  organization={IEEE}
}

@inproceedings{li2025rankexpert,
  title={Rankexpert: A mixture of textual-and-behavioral experts for multi-objective learning-to-rank in web search},
  author={Li, Yuchen and Zhang, Hao and Zhang, Yongqi and Cai, Hengyi and Cai, Mingxin and Wang, Shuaiqiang and Xiong, Haoyi and Kong, Linghe and Yin, Dawei and Chen, Lei},
  booktitle={Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2},
  pages={4578--4589},
  year={2025}
}

@article{li2025s,
  title={S3PRank: Towards Satisfaction-oriented Learning to Rank with Semi-supervised Pre-training},
  author={Li, Yuchen and Lyu, Zhonghao and Zhang, Yongqi and Zhang, Hao and Peng, Tianhao and Xiong, Haoyi and Wang, Shuaiqiang and Kong, Linghe and Chen, Guihai and Yin, Dawei},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2025},
  publisher={IEEE}
}

@inproceedings{li2025rankelectra,
  title={Rankelectra: Semi-supervised pre-training of learning-to-rank electra for web-scale search},
  author={Li, Yuchen and Xiong, Haoyi and Zhang, Yongqi and Bian, Jiang and Peng, Tianhao and Li, Xuhong and Wang, Shuaiqiang and Kong, Linghe and Yin, Dawei},
  booktitle={Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 1},
  pages={2415--2425},
  year={2025}
}

@article{li2023mhrr,
  title={Mhrr: Moocs recommender service with meta hierarchical reinforced ranking},
  author={Li, Yuchen and Xiong, Haoyi and Kong, Linghe and Zhang, Rui and Xu, Fanqin and Chen, Guihai and Li, Minglu},
  journal={IEEE Transactions on Services Computing},
  volume={16},
  number={6},
  pages={4467--4480},
  year={2023},
  publisher={IEEE}
}

@inproceedings{li2023ltrgcn,
  title={Ltrgcn: Large-scale graph convolutional networks-based learning to rank for web search},
  author={Li, Yuchen and Xiong, Haoyi and Kong, Linghe and Wang, Shuaiqiang and Sun, Zeyi and Chen, Hongyang and Chen, Guihai and Yin, Dawei},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={635--651},
  year={2023},
  organization={Springer}
}

@article{li2023coltr,
  title={COLTR: Semi-supervised learning to rank with co-training and over-parameterization for web search},
  author={Li, Yuchen and Xiong, Haoyi and Wang, Qingzhong and Kong, Linghe and Liu, Hao and Li, Haifang and Bian, Jiang and Wang, Shuaiqiang and Chen, Guihai and Dou, Dejing and others},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={35},
  number={12},
  pages={12542--12555},
  year={2023},
  publisher={IEEE}
}

@inproceedings{li2025fultr,
  title={Fultr: A large-scale fusion learning to rank dataset and its application for satisfaction-oriented ranking},
  author={Li, Yuchen and Zhang, Hao and Zhang, Haojie and Cai, Hengyi and Ma, Xinyu and Wang, Shuaiqiang and Xiong, Haoyi and Ren, Zhaochun and de Rijke, Maarten and Yin, Dawei},
  booktitle={Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2},
  pages={5583--5594},
  year={2025}
}

@inproceedings{li2023s2phere,
  title={S2phere: Semi-supervised pre-training for web search over heterogeneous learning to rank data},
  author={Li, Yuchen and Xiong, Haoyi and Kong, Linghe and Wang, Qingzhong and Wang, Shuaiqiang and Chen, Guihai and Yin, Dawei},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={4437--4448},
  year={2023}
}

@inproceedings{lu2025dmmd4sr,
  title={DMMD4SR: Diffusion Model-based Multi-level Multimodal Denoising for Sequential Recommendation},
  author={Lu, Weihai and Yin, Li},
  booktitle={Proceedings of the 33rd ACM International Conference on Multimedia},
  pages={6363--6372},
  year={2025}
}


@inproceedings{cui2025multi,
  title={Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising},
  author={Cui, Xiaoxi and Lu, Weihai and Tong, Yu and Li, Yiheng and Zhao, Zhejun},
  booktitle={Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={1593--1602},
  year={2025}
}

@inproceedings{cui2025diffusion,
  title={Diffusion-based multi-modal synergy interest network for click-through rate prediction},
  author={Cui, Xiaoxi and Lu, Weihai and Tong, Yu and Li, Yiheng and Zhao, Zhejun},
  booktitle={Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={581--591},
  year={2025}
}

@inproceedings{liu2025mdn,
  title={MDN: Modality Decomposition Network for Multimodal Recommendation},
  author={Liu, Zhuoyang and Lu, Weihai},
  booktitle={Proceedings of the 2025 International Conference on Multimedia Retrieval},
  pages={871--879},
  year={2025}
}

@article{mo2025one,
  title={One multimodal plugin enhancing all: CLIP-based pre-training framework enhancing multimodal item representations in recommendation systems},
  author={Mo, Minghao and Lu, Weihai and Xie, Qixiao and Xiao, Zikai and Lv, Xiang and Yang, Hong and Zhang, Yanchun},
  journal={Neurocomputing},
  volume={637},
  pages={130059},
  year={2025},
  publisher={Elsevier}
}

@inproceedings{tong2025dapt,
  title={DAPT: Domain-Aware Prompt-Tuning for Multimodal Fake News Detection},
  author={Tong, Yu and Lu, Weihai and Cui, Xiaoxi and Mao, Yifan and Zhao, Zhejun},
  booktitle={Proceedings of the 33rd ACM International Conference on Multimedia},
  pages={7902--7911},
  year={2025}
}

@inproceedings{wei2025igniting,
  title={Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards},
  author={Wei, Xiaolong and Lu, Bo and Zhang, Xingyu and Zhao, Zhejun and Shen, Dongdong and Xia, Long and Yin, Dawei},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  pages={17171--17197},
  year={2025}
}

@article{liao2024tpo,
  title={TPO: Aligning large language models with multi-branch \& multi-step preference trees},
  author={Liao, Weibin and Chu, Xu and Wang, Yasha},
  journal={arXiv preprint arXiv:2410.12854},
  year={2024}
}

@inproceedings{liao2023mcrle,
  title={MCRLe: Multi-Modal Contrastive Representation Learning For Stroke Onset Time Diagnosis},
  author={Liao, Weibin and Jiang, Peirong and Lv, Yi and Xue, Yunjing and Chen, Zhensen and Li, Xuesong},
  booktitle={2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

